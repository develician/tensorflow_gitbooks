# 인공신경망(ANN: Artificial Neural Network)

1. 뇌를 구성하는 신경세포, 즉 뉴런의 동작 원리에 기초. 
2. 신호 (즉: 역치), 역치가 너무 약하면 전달되지 않음. 혹은 강하면 강하게 말단으로 전달. 
3. 입력값: X 에 가중치: W 을 곱하고, 편향: b 를 더하여 활성화 함수(Sigmoid, ReLU) 등을 거쳐 결과값: y를 만들어 내는 것이 인공 뉴런의 기본. 

```python
y = Sigmoid(X * W + b)
출력 = 활성화함수(입력 * 가중치 + 편향)
W 와 b를 찾아내는것이 학습
```

# 활성화 함수 (Activation Function)

활성화 함수는 인공신경망을 통과해온 값을 최종적으로 어떤 값으로 만들지를 결정한다. 
이 함수가 바로 인공 뉴런의 핵심 중에서도 가장 중요한 요소. 

## 종류. 
대표적으로 Sigmoid, ReLU, tanh(쌍곡 탄젠트) 등이 있다.

1. Sigmoid
    * Sigmoid 그래프는 0과 1.0 (음수가 없다)
    * 그래프에서는 0 혹은 1.0 으로 수렴되는 것으로 보이지만 사실상 한없이 가까워지는것일뿐이다. 
    * ![Sigmoid](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)
2. ReLU
    * 최근 가장 대표적으로 많이 사용하는 활성화 함수.
    * 입력값이 0보다 작으면 항상 0을, 0보다 크면 입력값을 그대로 출력. 
    * ![ReLU & Sigmoid](https://www.researchgate.net/profile/Meng_Cai22/publication/261309983/figure/fig1/AS:614045627478053@1523411294935/Illustration-of-sigmoid-and-ReLU-nonlinearity.png)
3. tanh
    * 1.0과 -1.0 사이를 취급(음수 존재)
    * 그래프에서는 -1.0 혹은 1.0 으로 수렴되는 것으로 보이지만 사실상 한없이 가까워지는것일뿐이다. 
    * ![Sigmoid & tanh](https://cdn-images-1.medium.com/max/1600/1*f9erByySVjTjohfFdNkJYQ.jpeg)


# 정리 
인공 뉴런은 가중치와 활성화 함수의 연결로 이루어진 매우간단한 구조. 
이렇게 간단한 개념의 인공 뉴런을 충분히 많이 연결해놓는 것만으로도 인간이 인지하기 어려운 매우 복잡한 패턴까지도 스스로 학습할수 있게 된다.

![ANN](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1200px-Colored_neural_network.svg.png)

은닉층에서는 인공 뉴런이 작용하여 값을 만든다. Sigmoid(X * W + b)

# 역전파 (backpropagation)

* 역전파: 출력층이 내놓은 결과의 오차를 신경망을 따라 입력층까지 역으로 전파하여 계산하는 방식. 
* 이 방식은 입력층 부터 가중치를 조절해가는 기존 방식보다 훨씬 유의미한 방식으로 가중치를 조절해주어, 최적화 과정이 훨씬 빠르고 정확해진다.

1. 기존 학습 (기본학습)방법 
    * 모든 조합의 경우의 수에 대해 가중치를 대입하고 계산. 
2. 역전파
    * 결과값의 오차를 앞쪽으로 전파하면서 가중치를 갱신 